{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aac43c0",
   "metadata": {},
   "source": [
    "# Convolution Neural Network\n",
    "```\n",
    "INPUT -> [(Filter + Activation + Padding) -> Pooling] X n -> FCLayer -> OUTPUT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085d4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4f4883f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC : tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "Filter : tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]], grad_fn=<SelectBackward0>)\n",
      "Bias : tensor([10.], grad_fn=<SelectBackward0>)\n",
      "Convolution Layer size: torch.Size([125, 64])\n"
     ]
    }
   ],
   "source": [
    "# Conv\n",
    "batch_size = 10\n",
    "\n",
    "img_size = 128\n",
    "img_channel = 3\n",
    "\n",
    "stride = 1\n",
    "filter_size = 3\n",
    "filter_channel = 64\n",
    "conv_channel = filter_channel\n",
    "\n",
    "pool_size = (2, 2)\n",
    "pool_stride = 2\n",
    "\n",
    "output_category = 10\n",
    "\n",
    "src = torch.ones(batch_size, img_size, img_size, img_channel, dtype=torch.float32) \n",
    "print(f'SRC : {src[0, ..., 0]}')\n",
    "\n",
    "W = torch.full((filter_size, filter_size, filter_channel), 2, requires_grad = True, dtype=torch.float32)\n",
    "b = torch.full((1, filter_channel), 10, dtype=torch.float32, requires_grad=True) # , channel\n",
    "print(f'Filter : {W[..., 0]}')\n",
    "print(f'Bias : {b[:, 0]}')\n",
    "\n",
    "assert (img_size - filter_size) % stride == 0, \"Image, filter, and stride are not matched\"\n",
    "conv_size = (img_size - filter_size) // stride\n",
    "conv = torch.zeros(conv_size, conv_size, conv_channel, dtype=torch.float32)\n",
    "print(f'Convolution Layer size: {conv[0].shape}')\n",
    "\n",
    "\n",
    "def calculate_convolution(img, W, b, stride):\n",
    "    '''\n",
    "    img : [height, width, channel]\n",
    "    W : [filter_height, filter_width, filter_channel]\n",
    "    b : [1, filter_channel]\n",
    "    cnv : [conv_height, conv_width, conv_channel(=filter_channel)]\n",
    "    stride : the moving step\n",
    "    '''\n",
    "    img_height, img_width, img_channel = img.shape\n",
    "    filter_height, filter_width, filter_channel = W.shape\n",
    "    \n",
    "    assert (img_height - filter_height) % stride == 0, \"Image, filter, and stride are not matched\"\n",
    "    assert (img_width - filter_height) % stride == 0, \"Image, filter, and stride are not matched\"\n",
    "    \n",
    "    conv_height = (img_height - filter_height) // stride\n",
    "    conv_width = (img_width - filter_width) // stride\n",
    "    conv_channel = filter_channel\n",
    "    \n",
    "    conv = torch.zeros(conv_height, conv_width, conv_channel, dtype=torch.float32)\n",
    "    \n",
    "    for y in range(0, conv_height, stride):\n",
    "        for x in range(0, conv_width, stride):\n",
    "            for channel in range(img_channel):\n",
    "                conv[y,x,:] += (img[y:y+filter_height,x:x+filter_width,channel] @ W + b)\\\n",
    "                    .flatten(start_dim=0, end_dim=-2).sum(dim=0)\n",
    "                conv[y,x,:] = F.leaky_relu(conv[y,x,:])\n",
    "    return conv\n",
    "\n",
    "def pad_zeros(conv, img_size):\n",
    "    '''\n",
    "    conv : [conv_height, conv_width, conv_channel]\n",
    "    img_size : [img_height, img_width]\n",
    "    padded_conv : [img_height, img_width, conv_channel]\n",
    "    '''\n",
    "    conv_h, conv_w, conv_c = conv.shape\n",
    "    img_h, img_w = img_size\n",
    "    assert img_h >= conv_h and img_w >= conv_w, \"Convolution map is bigger than original\"\n",
    "    \n",
    "    gap_h = img_h - conv_h\n",
    "    gap_w = img_w - conv_w\n",
    "    \n",
    "    pad_upper = gap_h // 2 + 1\n",
    "    pad_lower = gap_h - pad_upper\n",
    "    pad_left = gap_w // 2 + 1\n",
    "    pad_right = gap_w - pad_left\n",
    "    \n",
    "    padded_conv = torch.zeros(img_h, img_w, conv_c, dtype=conv.dtype).to(conv.device)\n",
    "    padded_conv[pad_upper:img_h-pad_lower, pad_left:img_w-pad_right] = conv\n",
    "    return padded_conv\n",
    "\n",
    "def max_pooling(conv, pool_size, pool_stride):\n",
    "    '''\n",
    "    conv : [conv_height, conv_width, conv_channel]\n",
    "    pool_size : [pool_height, pool_width]\n",
    "    pool_stride : the moving step\n",
    "    pooled_conv : [pooled_height, pooled_width, conv_channel]\n",
    "    '''\n",
    "    conv_height, conv_width, conv_channel = conv.shape\n",
    "    pool_height, pool_width = pool_size\n",
    "    \n",
    "    assert (conv_height - pool_height) % pool_stride == 0, \"Conv, pool, and stride are not matched\"\n",
    "    assert (conv_width - pool_width) % pool_stride == 0, \"Conv, pool, and stride are not matched\"\n",
    "    \n",
    "    pooled_height = (conv_height - pool_height) // pool_stride\n",
    "    pooled_width = (conv_width - pool_width) // pool_stride\n",
    "    \n",
    "    pooled_conv = torch.zeros(pooled_height, pooled_width, conv_channel, dtype=conv.dtype).to(conv.device)\n",
    "    \n",
    "    for y in range(0, pooled_height, pool_stride):\n",
    "        for x in range(0, pooled_width, pool_stride):\n",
    "                pooled_conv[y,x,:] = conv[y:y+pooled_height, x:x+pooled_width].flatten(0, -2).max(dim=0).values\n",
    "    return pooled_conv\n",
    "\n",
    "def fc_layer(conv, output_category):\n",
    "    '''\n",
    "    conv : [conv_height, conv_width, conv_channel]\n",
    "    output_category : The number of outputs\n",
    "    '''\n",
    "    flatten_conv = conv.flatten()\n",
    "    \n",
    "    in_size = flatten_conv.shape[0]\n",
    "    out_size = output_category\n",
    "    \n",
    "    W = torch.rand(out_size, in_size, requires_grad=True).to(conv.device)\n",
    "    b = torch.rand(1, requires_grad=True).to(conv.device)\n",
    "    \n",
    "    output = W @ flatten_conv + b\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8f4464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = calculate_convolution(src[0], W, b, stride)\n",
    "padded_conv = pad_zeros(conv, src[0,...,0].shape)\n",
    "pooled_conv = max_pooling(padded_conv, pool_size, pool_stride)\n",
    "output = fc_layer(pooled_conv, output_category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
