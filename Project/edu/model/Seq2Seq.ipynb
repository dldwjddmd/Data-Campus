{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0f15e5",
   "metadata": {},
   "source": [
    "# Seq2Seq with torch and torchtext\n",
    "https://github.com/bentrevett/pytorch-seq2seq   \n",
    "```English to German```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a0754",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc4b8973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdata.datapipes.iter import IterDataPipe\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1572c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2226ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa186ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    '''\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    '''\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "def tokenize_en(text):\n",
    "    '''\n",
    "    Tokenizes English text from a string into a list of strings(tokens)\n",
    "    '''\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48836f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teddy/miniforge3/envs/Data/lib/python3.10/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "/Users/teddy/miniforge3/envs/Data/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = Multi30k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62646242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teddy/miniforge3/envs/Data/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:180: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from itertools import tee, zip_longest\n",
    "from functools import reduce\n",
    "\n",
    "def yield_corpus(corpus):\n",
    "    for src, trg in corpus:\n",
    "        srcTokens = tokenize_de(src)\n",
    "        trgTokens = tokenize_en(trg)\n",
    "        yield srcTokens, trgTokens\n",
    "    return\n",
    "\n",
    "# Reverse Order\n",
    "def yield_src(corpusIter):\n",
    "    for src, _ in corpusIter:\n",
    "        src.insert(0, '<sos>')\n",
    "        src.append('<eos>')\n",
    "        yield src\n",
    "    return\n",
    "        \n",
    "\n",
    "def yield_trg(corpusIter):\n",
    "    for _, trg in corpusIter:\n",
    "        trg.insert(0, '<sos>')\n",
    "        trg.append('<eos>')\n",
    "        yield trg\n",
    "    return\n",
    "\n",
    "srcCorpusIter, trgCorpusIter = tee(yield_corpus(train_data))\n",
    "srcIter, srcIterForVocab = tee(yield_src(srcCorpusIter))\n",
    "trgIter, trgIterForVocab = tee(yield_trg(trgCorpusIter))\n",
    "\n",
    "srcPadSize = reduce(lambda x, y: x if x > len(y) else len(y), srcIter, -1) + 2\n",
    "trgPadSize = reduce(lambda x, y: x if x > len(y) else len(y), trgIter, -1) + 2\n",
    "\n",
    "srcVocab = build_vocab_from_iterator(srcIterForVocab, min_freq = 2, special_first=True, specials=['<unk>', '<sos>', '<eos>', '<pad>'])\n",
    "srcVocab.set_default_index(srcVocab['<unk>'])\n",
    "trgVocab = build_vocab_from_iterator(trgIterForVocab, min_freq = 2, special_first=True, specials=['<unk>', '<sos>', '<eos>', '<pad>'])\n",
    "trgVocab.set_default_index(trgVocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3405ee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(srcPadSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3db798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.map import SequenceWrapper, Mapper\n",
    "from functools import reduce\n",
    "from copy import deepcopy\n",
    "\n",
    "def build_src(src):\n",
    "    src = deepcopy(src)\n",
    "    src.insert(0, '<eos>')\n",
    "    src.append('<sos>')\n",
    "    return src\n",
    "\n",
    "def build_trg(trg):\n",
    "    trg = deepcopy(trg)\n",
    "    trg.insert(0, '<sos>')\n",
    "    trg.append('<eos>')\n",
    "    return trg\n",
    "\n",
    "def do_integer_encoding(pair):\n",
    "    src, trg = pair\n",
    "    encodedSrc = srcVocab.forward(src)\n",
    "    encodedTrg = trgVocab.forward(trg)\n",
    "    paddedSrc = encodedSrc + [srcVocab['<pad>'] for _ in range(srcPadSize - len(encodedSrc))]\n",
    "    paddedTrg = encodedTrg + [trgVocab['<pad>'] for _ in range(trgPadSize - len(encodedTrg))]\n",
    "    return torch.LongTensor(paddedSrc).to(device), torch.LongTensor(paddedTrg).to(device)\n",
    "\n",
    "def build_corpus(pair):\n",
    "    src, trg = deepcopy(pair)\n",
    "    src = build_src(tokenize_de(src))\n",
    "    trg = build_trg(tokenize_en(trg))\n",
    "    return src, trg\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "trainDP = SequenceWrapper(list(train_data)).map(build_corpus).map(do_integer_encoding)\n",
    "validDP = SequenceWrapper(list(valid_data)).map(build_corpus).map(do_integer_encoding)\n",
    "testDP = SequenceWrapper(list(test_data)).map(build_corpus).map(do_integer_encoding)\n",
    "\n",
    "\n",
    "\n",
    "trainLoader = DataLoader(trainDP, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, )\n",
    "validLoader = DataLoader(validDP, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, )\n",
    "testLoader = DataLoader(testDP, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3140b",
   "metadata": {},
   "source": [
    "## Build Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e1e2d",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "- 2-Layers\n",
    "- LSTM returns ```outputs```, ```hidden```, and ```cell```\n",
    "- To make context vector, only need ```hidden``` and ```cell```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "288bd8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, )\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # src: [src len, batch size]\n",
    "        embedded = self.dropout(self.embedding(src)) # [src len, batch size, emb_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # output = last hidden states, [src len, batch size, hid dim * n directions]\n",
    "        # hidden = final hidden states, [n layers * n directions, batch size, hid dim]\n",
    "        # cell = final cell states, [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2ee27",
   "metadata": {},
   "source": [
    "### Context Vector\n",
    "- At each layer, context vector is created by ```cell``` and ```hidden```\n",
    "- If the number of layers at encoder and decoder are different, We make decision like passing same context vector? etc.\n",
    "\n",
    "### Decoder\n",
    "- Teacher force is used probabilitly\n",
    "- Loop decoder until entering ```<eos>```\n",
    "- Decoder poop starts at ```1```, not 0. So, \n",
    "```\n",
    "trg = [<sos>, y1, y2, y3, <eos>]\n",
    "pred = [0, p1, p2, p3, <eos>]\n",
    "```\n",
    "- Weh ncalculate the loss, cur off the first element of each tensor to get\n",
    "```\n",
    "trg = [y1, y2, y3, <eos>]\n",
    "pred = [p1, p2, p3, <eos>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d87788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim ,emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout, batch_first = True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, input_, hidden, cell):\n",
    "        #input = [batchsize, token_size] <- a token [128]\n",
    "        #hidden, cell <- passed by encoder first, [2, 128, 256]\n",
    "        \n",
    "        #Decoder's hidden, cell : [n layers * n directions, batch size, hid dim]\n",
    "        input_ = input_.unsqueeze(1) #Make token a seq, [batch_size, 1]\n",
    "        embedded = self.dropout(self.embedding(input_)) # [batch_size, 1, emb dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell)) # With hidden and cell, make context\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0)) # [batch size, output dim]\n",
    "        return prediction, hidden, cell # Re-passed by parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c33afeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimentions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layeres!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        #src: [batch size, src len]\n",
    "        #trg: [batch size, trg len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #Store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[:, 0] #first input to the decoder is the <sos> tokens\n",
    "        \n",
    "        for t in range(1, trg_len): # output[0:] is zeros!!!!\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:,t,:] = output.squeeze()\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(0) # Find best prediction at each sample, [trg_len]\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31baa22",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50de787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(srcVocab)\n",
    "OUTPUT_DIM = len(trgVocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db33be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8014"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(srcVocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a97965",
   "metadata": {},
   "source": [
    "### Weight normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd227ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(8014, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(6191, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=6191, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, +0.08)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57154d",
   "metadata": {},
   "source": [
    "### Count the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7f60fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 14,168,879 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(mode):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "482c7a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "TRG_PAD_IDX = trgVocab['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c87e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train() # Set train mode <--> model.eval() : Set prediction mode with torch.no_grad() : Don't calcuate gradients\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, trg = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1] # [trg len, batch size, output dim]\n",
    "        output = output[1:].view(-1, output_dim) # Ignore first\n",
    "        trg = trg[1:].view(-1) # Ignore first\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b833032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, trg = batch\n",
    "            output = model(src, trg, 0) # turn off teacher forcing\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55a891a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins*60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de49f978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_secs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     best_valid_loss \u001b[38;5;241m=\u001b[39m valid_loss\n\u001b[1;32m     16\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtut1-model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_mins\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_secs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train PPL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath\u001b[38;5;241m.\u001b[39mexp(train_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Val. Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val. PPL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath\u001b[38;5;241m.\u001b[39mexp(valid_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_secs' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, trainLoader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, validLoader, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss: .3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8965c942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 8.281 | Test PPL: 3950.037 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "test_loss = evaluate(model, testLoader, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
